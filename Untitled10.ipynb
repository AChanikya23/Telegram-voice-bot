{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install all required packages\n",
        "!pip install python-telegram-bot requests nest_asyncio python-dotenv gTTS pydub git+https://github.com/openai/whisper.git --quiet\n",
        "!apt-get install ffmpeg -y\n",
        "\n",
        "# Step 2: Imports\n",
        "import os\n",
        "import whisper\n",
        "import requests\n",
        "import nest_asyncio\n",
        "from telegram import Update\n",
        "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# Step 3: Init async\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Step 4: Load Whisper model (base is fast and accurate)\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Step 5: Set tokens manually (or use dotenv if preferred)\n",
        "HF_TOKEN = \"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\"         # â¬…ï¸ Replace this\n",
        "TELEGRAM_TOKEN = \"7903691763:AAE7s-OUpYMTkTYiOE8AtXZ4pkRHMcAQ4_M\"      # â¬…ï¸ Replace this\n",
        "\n",
        "HF_MODEL = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "HF_URL = f\"https://api-inference.huggingface.co/models/{HF_MODEL}\"\n",
        "HF_HEADERS = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "\n",
        "# Step 6: LLM text generation via Hugging Face\n",
        "def query_huggingface(prompt):\n",
        "    payload = {\n",
        "        \"inputs\": f\"<|system|>You are a helpful assistant.<|user|>{prompt}<|assistant|>\",\n",
        "        \"parameters\": {\"max_new_tokens\": 200, \"temperature\": 0.7},\n",
        "    }\n",
        "    response = requests.post(HF_URL, headers=HF_HEADERS, json=payload)\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            return response.json()[0]['generated_text'].split(\"<|assistant|>\")[-1].strip()\n",
        "        except Exception:\n",
        "            return \"âš ï¸ Error parsing response.\"\n",
        "    else:\n",
        "        print(\"HuggingFace API Error:\", response.text)\n",
        "        return \"âš ï¸ Sorry, the model is currently unavailable.\"\n",
        "\n",
        "# Step 7: Local transcription using Whisper\n",
        "def transcribe_voice(file_path):\n",
        "    try:\n",
        "        result = whisper_model.transcribe(file_path)\n",
        "        return result[\"text\"]\n",
        "    except Exception as e:\n",
        "        print(\"Whisper Error:\", str(e))\n",
        "        return \"âš ï¸ Local transcription failed.\"\n",
        "\n",
        "# Step 8: Convert reply text to voice\n",
        "def generate_voice(text, filename=\"response.mp3\"):\n",
        "    tts = gTTS(text)\n",
        "    tts.save(filename)\n",
        "    return filename\n",
        "\n",
        "# Step 9: Telegram bot logic\n",
        "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    await update.message.reply_text(\"ğŸ‘‹ Send me a text or voice message, and Iâ€™ll reply with voice!\")\n",
        "\n",
        "async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    user_input = update.message.text.strip()\n",
        "    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=\"typing\")\n",
        "    reply = query_huggingface(user_input)\n",
        "    voice_file = generate_voice(reply)\n",
        "    await update.message.reply_voice(voice=open(voice_file, \"rb\"))\n",
        "\n",
        "async def handle_voice(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    file = await update.message.voice.get_file()\n",
        "    voice_path_ogg = \"voice.ogg\"\n",
        "    voice_path_mp3 = \"voice.mp3\"\n",
        "    await file.download_to_drive(voice_path_ogg)\n",
        "\n",
        "    try:\n",
        "        sound = AudioSegment.from_ogg(voice_path_ogg)\n",
        "        sound.export(voice_path_mp3, format=\"mp3\")\n",
        "\n",
        "        text = transcribe_voice(voice_path_mp3)\n",
        "        await update.message.reply_text(f\"ğŸ“ You said: {text}\")\n",
        "\n",
        "        reply = query_huggingface(text)\n",
        "        voice_reply = generate_voice(reply)\n",
        "        await update.message.reply_voice(voice=open(voice_reply, \"rb\"))\n",
        "    except Exception as e:\n",
        "        print(\"Voice handling error:\", str(e))\n",
        "        await update.message.reply_text(\"âŒ I couldnâ€™t process your voice message.\")\n",
        "\n",
        "# Step 10: Launch bot\n",
        "app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()\n",
        "app.add_handler(CommandHandler(\"start\", start))\n",
        "app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))\n",
        "app.add_handler(MessageHandler(filters.VOICE, handle_voice))\n",
        "\n",
        "print(\"ğŸ™ï¸ Voice-enabled Telegram Bot with Local Whisper is running...\")\n",
        "app.run_polling()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjjTjQhd_XPP",
        "outputId": "0c740987-ec96-42d8-a877-33f05408bdf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m673.5/673.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 34 not upgraded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:01<00:00, 101MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ™ï¸ Voice-enabled Telegram Bot with Local Whisper is running...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N-Tjz0-V_hXm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}